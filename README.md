Set up Ollama and use a model, e.g.:

```
ollama pull llama3.2
ollama serve
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Run the app:

```
streamlit run app.py
```
